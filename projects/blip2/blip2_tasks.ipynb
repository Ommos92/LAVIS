{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/ommos92/.cache/lavis/msvd/annotations/qa_train.json\n",
      "Using downloaded and verified file: /home/ommos92/.cache/lavis/msvd/annotations/qa_val.json\n",
      "Using downloaded and verified file: /home/ommos92/.cache/lavis/msvd/annotations/qa_test.json\n",
      "Using downloaded and verified file: /home/ommos92/.cache/lavis/msvd/annotations/qa_ans2label.json\n",
      "dict_keys(['train', 'val', 'test'])\n",
      "30933\n",
      "{'video': '/home/ommos92/.cache/lavis/msvd/videos/-4wsuPCjDBc_5_15.avi', 'text_input': 'what is chewing on a nut?', 'answers': 35, 'question_id': '0000000', 'instance_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "from lavis.datasets.builders import load_dataset\n",
    "msvd_dataset = load_dataset(\"msvd_qa\")\n",
    "\n",
    "print(msvd_dataset.keys())\n",
    "print(len(msvd_dataset[\"train\"]))\n",
    "print(msvd_dataset[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "#Write a function to convert the .avi into a set of images. Then we can randomly \n",
    "# sample from this set of images.\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def convert_avi(data):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(data['video'])\n",
    "\n",
    "    images = []\n",
    "    while(cap.isOpened()):\n",
    "        # Read the video frame by frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Convert the frame to RGB (OpenCV uses BGR by default)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            resized_frame = cv2.resize(frame, (224,224))\n",
    "            # Convert the frame to a PIL Image and append to the list\n",
    "            images.append(Image.fromarray(resized_frame))\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Close the video file\n",
    "    cap.release()\n",
    "\n",
    "    # Return the list of PIL Images\n",
    "    return images\n",
    "\n",
    "def random_sample(images, num_samples):\n",
    "    #Randomly select 'num_samples' images from the list\n",
    "    sampled_images = random.sample(images, num_samples)\n",
    "    return sampled_images\n",
    "\n",
    "\n",
    "images = convert_avi(msvd_dataset[\"train\"][0])\n",
    "print(len(images))\n",
    "\n",
    "# Show the shope of the image\n",
    "print(images[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup device to use\n",
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.63s/it]\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "model, vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", is_eval=True, device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m vqa_task \u001b[38;5;241m=\u001b[39m VQATask(\n\u001b[1;32m      5\u001b[0m     num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mvqa_task\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsvd_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAVIS/lavis/tasks/vqa.py:98\u001b[0m, in \u001b[0;36mVQATask.valid_step\u001b[0;34m(self, model, samples)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalid_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, samples):\n\u001b[0;32m---> 98\u001b[0m     answers \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_answers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_ans_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_ans_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     pred_qa_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    110\u001b[0m     question_id \u001b[38;5;241m=\u001b[39m samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/LAVIS/lavis/models/blip2_models/blip2_opt.py:295\u001b[0m, in \u001b[0;36mBlip2OPT.predict_answers\u001b[0;34m(self, samples, num_beams, inference_method, max_len, min_len, num_ans_candidates, answer_list, prompt, length_penalty, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_answers\u001b[39m(\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    284\u001b[0m     samples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    294\u001b[0m ):\n\u001b[0;32m--> 295\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_autocast():\n\u001b[1;32m    297\u001b[0m         image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_vision(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_encoder(image))\n",
      "File \u001b[0;32m~/LAVIS/lavis/datasets/datasets/video_vqa_datasets.py:49\u001b[0m, in \u001b[0;36mVideoQADataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_labels\n\u001b[1;32m     47\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not built yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m     ann \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     51\u001b[0m     vname \u001b[38;5;241m=\u001b[39m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m     vpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvis_root, vname)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# from lavis.tasks.vqa import VQATask\n",
    "\n",
    "# # Create a VQATask instance\n",
    "# vqa_task = VQATask(\n",
    "#     num_beams=3,\n",
    "#     max_len=10,\n",
    "#     min_len=1,\n",
    "#     evaluate=True,\n",
    "#     num_ans_candidates=128,\n",
    "#     inference_method=\"rank\",\n",
    "#     prompt=\"\"\n",
    "# )\n",
    "\n",
    "# # Generate predictions\n",
    "# predictions = vqa_task.valid_step(model, samples=msvd_dataset[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'getQuesIds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m ground_truth_question \u001b[38;5;241m=\u001b[39m annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Ensure the predicted question matches the ground truth question\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#assert question == ground_truth_question, \"Predicted question does not match ground truth question\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Create an instance of VQAEval\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m vqa_eval \u001b[38;5;241m=\u001b[39m \u001b[43mVQAEval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_answer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Compute the accuracy\u001b[39;00m\n\u001b[1;32m     34\u001b[0m vqa_eval\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/LAVIS/lavis/common/vqa_tools/vqa_eval.py:28\u001b[0m, in \u001b[0;36mVQAEval.__init__\u001b[0;34m(self, vqa, vqaRes, n)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqaRes \u001b[38;5;241m=\u001b[39m vqaRes\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vqa \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mvqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetQuesIds\u001b[49m()}\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontractions \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124main\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maren\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myouve\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    150\u001b[0m }\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanualMap \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mten\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    164\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'getQuesIds'"
     ]
    }
   ],
   "source": [
    "from lavis.common.vqa_tools.vqa_eval import VQAEval\n",
    "\n",
    "\n",
    "# Initialize a list to store the accuracy for each video\n",
    "accuracies = []\n",
    "\n",
    "# Loop over the videos in the dataset\n",
    "#for video in msvd_dataset[\"test\"]:\n",
    "for i, video in enumerate(msvd_dataset[\"test\"]):\n",
    "    # Sample a random frame from the video\n",
    "    frame = random_sample(convert_avi(video), 1)\n",
    "    image = vis_processors[\"eval\"](frame[0]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get the annotation for the video\n",
    "    annotation = msvd_dataset['test'].annotation[i]\n",
    "\n",
    "    # Perform VQA on the frame\n",
    "    predicted_answer = model.generate(\n",
    "        {\"image\" : image, \"question\" : annotation['question']}\n",
    "        )\n",
    "    \n",
    "    # Get the ground truth from the annotation\n",
    "    ground_truth_answer = annotation['answer']\n",
    "    ground_truth_question = annotation['question']\n",
    "    \n",
    "    \n",
    "    # Ensure the predicted question matches the ground truth question\n",
    "    #assert question == ground_truth_question, \"Predicted question does not match ground truth question\"\n",
    "    \n",
    "    # Create an instance of VQAEval\n",
    "    vqa_eval = VQAEval(ground_truth_answer, predicted_answer[0], n=2)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    vqa_eval.evaluate()\n",
    "    accuracy = vqa_eval.accuracy[\"overall\"]\n",
    "    \n",
    "    # Add the accuracy to the list\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Print the accuracy for this video\n",
    "    print(accuracy)\n",
    "\n",
    "# After the loop, you can do something with the list of accuracies, like compute the average accuracy\n",
    "average_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(f\"Average accuracy: {average_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
